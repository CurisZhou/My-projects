{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# 1. -*-  load fake news data (from Kaggle) -*-\n",
    "\n",
    "def load_data(file=\"fake_news_data.csv\"):\n",
    "    data = pd.read_csv(file,encoding=\"utf-8\")\n",
    "    # print(data.apply(lambda x: len(x)).head(6))\n",
    "\n",
    "    # omit all rows with null values\n",
    "    data = data.dropna()\n",
    "\n",
    "    # convert all non-string columns into string-type columns\n",
    "    data = data.astype({\"URLs\":\"str\",\"Headline\":\"str\",\"Body\":\"str\"})\n",
    "\n",
    "    # data_0 = data.loc[data[\"Label\"]==0]\n",
    "    # data_1 = data.loc[data[\"Label\"]==1]\n",
    "    # print(\"True news\",data_0.shape)\n",
    "    # print(\"Fake news\",data_1.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. -*-  Output accuracy/classification report/confusion matrix. Output the results of cross-validation -*-\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "def evaluation_predictions(true_labels,predictions):\n",
    "    print(\"\\nAccuracy score:\",accuracy_score(true_labels,predictions))\n",
    "    print(\"Classification report:\\n\",classification_report(true_labels,predictions))\n",
    "    print(\"Confusion matrix:\\n\",confusion_matrix(true_labels,predictions))\n",
    "\n",
    "# output the results of cross-validation\n",
    "def print_cv_scores_summary(cv_scores):\n",
    "    def cv_score_summary(name,scores):\n",
    "        print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}%, max = {:.2f}%\".format(name, scores.mean() * 100,\n",
    "                                                                                    scores.std() * 100,\n",
    "                                                                                    scores.min() * 100,\n",
    "                                                                                    scores.max() * 100))\n",
    "\n",
    "    cv_score_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "    cv_score_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "    cv_score_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "    cv_score_summary(\"F1\", cv_scores['test_f1_weighted'])\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  A couple of methods for showing classifier results\n",
    "def confusion_matrix_heatmap(true_labels,predictions,labels):\n",
    "    confu_matrix = confusion_matrix(true_labels,predictions)\n",
    "    cmdf = pd.DataFrame(confu_matrix, index = labels, columns=labels)\n",
    "    dims = (10, 10)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "def confusion_matrix_percent_heatmap(true_labels,predictions,labels):\n",
    "    confu_matrix = confusion_matrix(true_labels,predictions)\n",
    "    cmdf = pd.DataFrame(confu_matrix, index = labels, columns=labels)\n",
    "    percents = cmdf.div(cmdf.sum(axis=1), axis=0)*100\n",
    "    dims = (10, 10)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(percents, annot=True, cmap=\"coolwarm\", center=0, vmin=0, vmax=100)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_ticks([0, 25, 50, 75, 100])\n",
    "    cbar.set_ticklabels(['0%', '25%', '50%', '75%', '100%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. -*- Customising preprocessing -*-\n",
    "import ftfy\n",
    "\n",
    "def preprocess(text):\n",
    "    # preprocess the multi-spaces, newlines and urls in strings\n",
    "\n",
    "    multispace_re = re.compile(r\"\\s{2,}\")\n",
    "    newline_re = re.compile(r\"\\n+\")\n",
    "    url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "    # emoji_re = re.compile(\"([\\U00010000-\\U0010ffff]|[\\uD800-\\uDBFF][\\uDC00-\\uDFFF])\")\n",
    "\n",
    "    preprocessed_text = multispace_re.sub(\"\",text)\n",
    "    preprocessed_text = newline_re.sub(\"\",preprocessed_text)\n",
    "    preprocessed_text = url_re.sub(\"url\", preprocessed_text)\n",
    "    preprocessed_text = ftfy.fix_text(preprocessed_text)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. -*- Customising word tokenization and word lemmatization -*-\n",
    "\n",
    "def nltk_word_tokenize(text,advanced_tokenize=True):\n",
    "    if advanced_tokenize:\n",
    "        advanced_text_tokenize = nltk.tokenize.TweetTokenizer()\n",
    "        return advanced_text_tokenize.tokenize(text)\n",
    "    else:\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "# utilize WordNetLemmatizer class to creat the lemma n-gram\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# pos parameter could be \"n\", \"v\", \"a\", \"r\"\n",
    "def word_lemmatizer(word, pos = \"v\"):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. -*- Creating a document class to extract different features for each row of fake news data -*-\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams,skipgrams\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class Document():\n",
    "    def __init__(self,meta = {\"Web_source\":\"\"}):\n",
    "        self.meta = meta\n",
    "\n",
    "        # extract web source of news as a feature\n",
    "        self.web_source_dict = Counter()\n",
    "        self.web_source_dict.update(self.meta[\"Web_source\"])\n",
    "\n",
    "        # storing each token and corresponding pos-tag into lists\n",
    "        self.tokens_list = []\n",
    "        self.pos_tag_list = []\n",
    "        # count the number of tokens and assign the text to the variable self.text\n",
    "        self.num_tokens = 0\n",
    "        self.text = \"\"\n",
    "\n",
    "        # storing the frequencies of tokens into a statistical Counter class\n",
    "        self.token_frequencies_dict = Counter()\n",
    "        # storing the frequencies of pos-tags into a statistical Counter class\n",
    "        self.pos_tag_frequencies_dict = Counter()\n",
    "\n",
    "        # storing the frequencies of bi-gram into a statistical Counter class\n",
    "        self.bi_gram_frequencies_dict  = Counter()\n",
    "        # storing the frequencies of pos-tag bi-gram into a statistical Counter class\n",
    "        self.postag_bi_gram_frequencies_dict = Counter()\n",
    "        # storing the frequencies of lemma bi-gram into a statistical Counter class\n",
    "        self.lemma_bi_gram_frequencies_dict = Counter()\n",
    "        # storing the frequencies of bi-skipgram into a statistical Counter class\n",
    "        self.bi_skipgram_frequencies_dict = Counter()\n",
    "\n",
    "        # storing the frequencies of tri-gram into a statistical Counter class\n",
    "        self.tri_gram_frequencies_dict = Counter()\n",
    "        # storing the frequencies of pos-tag tri-gram into a statistical Counter class\n",
    "        self.postag_tri_gram_frequencies_dict = Counter()\n",
    "        # storing the frequencies of lemma tri-gram into a statistical Counter class\n",
    "        self.lemma_tri_gram_frequencies_dict = Counter()\n",
    "        # storing the frequencies of tri-skipgram into a statistical Counter class\n",
    "        self.tri_skipgram_frequencies_dict = Counter()\n",
    "\n",
    "    def extract_features_from_text(self,text):\n",
    "\n",
    "        preprocessed_text = preprocess(text)\n",
    "        self.text = preprocessed_text\n",
    "\n",
    "        # tokenization\n",
    "        tokens = nltk_word_tokenize(preprocessed_text)\n",
    "        self.tokens_list.extend(tokens)\n",
    "        self.num_tokens += len(tokens)\n",
    "        self.token_frequencies_dict.update(tokens)\n",
    "\n",
    "        # get the pos-tags of tokens\n",
    "        tokens_postag = nltk.pos_tag(tokens)\n",
    "        pos_tags = [pos_tag[1] for pos_tag in tokens_postag]\n",
    "        self.pos_tag_list.extend(pos_tags)\n",
    "        self.pos_tag_frequencies_dict.update(pos_tags)\n",
    "\n",
    "        # get the verb lemmatization forms of tokens\n",
    "        lemma_tokens = [word_lemmatizer(word,pos=\"v\") for word in self.tokens_list]\n",
    "\n",
    "        bi_gram_list = [\"_\".join(bigram) for bigram in ngrams(self.tokens_list,2)]\n",
    "        self.bi_gram_frequencies_dict.update(bi_gram_list)\n",
    "\n",
    "        postag_bi_gram_list = [\"_\".join(postag_bigram) for postag_bigram in ngrams(self.pos_tag_list,2)]\n",
    "        self.postag_bi_gram_frequencies_dict.update(postag_bi_gram_list)\n",
    "\n",
    "        lemma_bi_gram_list = [\"_\".join(lemma_bigram) for lemma_bigram in ngrams(lemma_tokens, 2)]\n",
    "        self.lemma_bi_gram_frequencies_dict.update(lemma_bi_gram_list)\n",
    "\n",
    "        # creat bi-skipgrams,and the distance of shipped tokens is 2 (containing bi-gram)\n",
    "        bi_skipgram_list = [\"_\".join(bi_skipgram) for bi_skipgram in skipgrams(self.tokens_list,2,2)]\n",
    "        self.bi_skipgram_frequencies_dict.update(bi_skipgram_list)\n",
    "\n",
    "\n",
    "        tri_gram_list = [\"_\".join(trigram) for trigram in ngrams(self.tokens_list, 3)]\n",
    "        self.tri_gram_frequencies_dict.update(tri_gram_list)\n",
    "\n",
    "        postag_tri_gram_list = [\"_\".join(postag_trigram) for postag_trigram in ngrams(self.pos_tag_list, 3)]\n",
    "        self.postag_tri_gram_frequencies_dict.update(postag_tri_gram_list)\n",
    "\n",
    "        lemma_tri_gram_list = [\"_\".join(lemma_trigram) for lemma_trigram in ngrams(lemma_tokens, 3)]\n",
    "        self.lemma_tri_gram_frequencies_dict.update(lemma_tri_gram_list)\n",
    "\n",
    "        # creat tri-skipgrams,and the distance of shipped tokens is 2 (containing tri-gram)\n",
    "        tri_skipgram_list = [\"_\".join(bi_skipgram) for bi_skipgram in skipgrams(self.tokens_list,3,2)]\n",
    "        self.tri_skipgram_frequencies_dict.update(tri_skipgram_list)\n",
    "\n",
    "\n",
    "    def extract_features_from_texts(self, texts):\n",
    "        for text in texts:\n",
    "            self.extract_features_from_text(text)\n",
    "\n",
    "    def average_token_length(self):\n",
    "        sum_character_length = 0\n",
    "        for word,frequency in self.token_frequencies_dict.items():\n",
    "            sum_character_length += len(word) * frequency\n",
    "        return sum_character_length / self.num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. -*- Creating training data and labels, and test data and labels -*-\n",
    "\n",
    "def corpus_creation(data_df):\n",
    "    for row in range(data_df.shape[0]):\n",
    "        # use regular expression to extract web source\n",
    "        web_source = re.search(r\"((?<=(www.|api.))[a-zA-Z0-9]{1,}|[a-zA-Z0-9]{1,}(?=(.com|.tv|.net|.it)))\",\n",
    "                                            data_df.iloc[row, 0]).group()\n",
    "        # store web source and label, of fake news, into the meta dictionary variable of Document class\n",
    "        doc = Document(meta={\"Web_source\": web_source, \"label\": data_df.iloc[row, 3]})\n",
    "\n",
    "        # combine news headline and news body\n",
    "        text = data_df.iloc[row, 1] + \" \" + data_df.iloc[row, 2]\n",
    "        doc.extract_features_from_text(text)\n",
    "        yield doc\n",
    "\n",
    "\n",
    "\n",
    "def train_text_data_creation(data_partition=True):\n",
    "    # creat corpus and labels\n",
    "    data_df = load_data()\n",
    "\n",
    "    corpus = []\n",
    "    corpus.extend(corpus_creation(data_df))\n",
    "    Y = [doc.meta[\"label\"] for doc in corpus]\n",
    "\n",
    "    if data_partition:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(corpus, Y, test_size=0.3, random_state=0)\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    else:\n",
    "        return corpus,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7. -*-\n",
    "# Now, the train/test instances are created, a custom Transformer is needed to be constructed, which takes in one dataset\n",
    "# and returns a new dataset. Here we need to take in a list of Document objects and transform it into a set of features.\n",
    "# We build a simple class for this, which overrides the transform method. The intention is for a list of Document objects\n",
    "# to be passed into the transformer, and parameter-defined (callable) method is used to extract featuress. -*-\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DocumentProcessor(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,process_method):\n",
    "        self.process_method = process_method\n",
    "\n",
    "    # no fitting necessary, although could use this to build a vocabulary for all documents, and then limit to set (e.g. top 1000).\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,documents):\n",
    "        for document in documents:\n",
    "            yield self.process_method(document)\n",
    "\n",
    "# Below are some process methods for returning the extracted features from the Document. These can be edited and added to as needed.\n",
    "def get_token_frequencies_dict(document):\n",
    "    return document.token_frequencies_dict\n",
    "\n",
    "def get_pos_tag_frequencies_dict(document):\n",
    "    return document.pos_tag_frequencies_dict\n",
    "\n",
    "def get_bi_gram_frequencies_dict(document):\n",
    "    return document.bi_gram_frequencies_dict\n",
    "\n",
    "def get_postag_bi_gram_frequencies_dict(document):\n",
    "    return document.postag_bi_gram_frequencies_dict\n",
    "\n",
    "def get_lemma_bi_gram_frequencies_dict(document):\n",
    "    return document.lemma_bi_gram_frequencies_dict\n",
    "\n",
    "def get_bi_skipgram_frequencies_dict(document):\n",
    "    return document.bi_skipgram_frequencies_dict\n",
    "\n",
    "def get_tri_gram_frequencies_dict(document):\n",
    "    return document.tri_gram_frequencies_dict\n",
    "\n",
    "def get_postag_tri_gram_frequencies_dict(document):\n",
    "    return document.postag_tri_gram_frequencies_dict\n",
    "\n",
    "def get_lemma_tri_gram_frequencies_dict(document):\n",
    "    return document.lemma_tri_gram_frequencies_dict\n",
    "\n",
    "def get_tri_skipgram_frequencies_dict(document):\n",
    "    return document.tri_skipgram_frequencies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8. -*- Construct different pipeline models based on combination of different ngram features -*-\n",
    "\n",
    "# pipeline model 1 is based on tokens features (bag of words)\n",
    "model_1 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"tokens_features\",Pipeline([\n",
    "            (\"tokens_processor\",DocumentProcessor(process_method=get_token_frequencies_dict)),\n",
    "            (\"tokens_vectorizer\",DictVectorizer())\n",
    "        ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 2 is based on pos-tag features (bag of pos-tags)\n",
    "model_2 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"pos_tag_features\",Pipeline([\n",
    "            (\"pos_tag_processor\",DocumentProcessor(process_method=get_pos_tag_frequencies_dict)),\n",
    "            (\"pos_tag_vectorizer\",DictVectorizer())\n",
    "        ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 3 is based on bi-gram and/or tri-gram\n",
    "model_3 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"bi_gram_features\",Pipeline([\n",
    "            (\"bi_gram_processor\",DocumentProcessor(process_method=get_bi_gram_frequencies_dict)),\n",
    "            (\"bi_gram_vectorizer\",DictVectorizer())\n",
    "        ])),\n",
    "#         (\"tri_gram_features\",Pipeline([\n",
    "#             (\"tri_gram_processor\",DocumentProcessor(process_method=get_tri_gram_frequencies_dict)),\n",
    "#             (\"tri_gram_vectorizer\",DictVectorizer())\n",
    "#         ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 4 is based on pos-tag bi-gram and/or pos-tag tri-gram\n",
    "model_4 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"postag_bi_gram_features\",Pipeline([\n",
    "            (\"postag_bi_gram_processor\",DocumentProcessor(process_method=get_postag_bi_gram_frequencies_dict)),\n",
    "            (\"postag_bi_gram_vectorizer\",DictVectorizer())\n",
    "        ])),\n",
    "        # (\"postag_tri_gram_features\",Pipeline([\n",
    "        #     (\"postag_tri_gram_processor\",DocumentProcessor(process_method=get_postag_tri_gram_frequencies_dict)),\n",
    "        #     (\"postag_tri_gram_vectorizer\",DictVectorizer())\n",
    "        # ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 5 is based on lemma bi-gram and/or lemma tri-gram\n",
    "model_5 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"lemma_bi_gram_features\",Pipeline([\n",
    "            (\"lemma_bi_gram_processor\",DocumentProcessor(process_method=get_lemma_bi_gram_frequencies_dict)),\n",
    "            (\"lemma_bi_gram_vectorizer\",DictVectorizer())\n",
    "        ])),\n",
    "        # (\"lemma_tri_gram_features\",Pipeline([\n",
    "        #     (\"lemma_tri_gram_processor\",DocumentProcessor(process_method=get_lemma_tri_gram_frequencies_dict)),\n",
    "        #     (\"lemma_tri_gram_vectorizer\",DictVectorizer())\n",
    "        # ])),\n",
    "    ])),\n",
    "    (\"clf\",LogisticRegression(random_state=0,solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "# pipeline model 6_1 is based on bi-skipgram (containing bi-gram )\n",
    "model_6_1 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"bi_skipgram_features\",Pipeline([\n",
    "            (\"bi_skipgram_processor\",DocumentProcessor(process_method=get_bi_skipgram_frequencies_dict)),\n",
    "            (\"bi_skipgram_vectorizer\",DictVectorizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 6_2 is based on tri-skipgram (containing tri-gram)\n",
    "model_6_2 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"tri_skipgram_features\",Pipeline([\n",
    "            (\"tri_skipgram_processor\",DocumentProcessor(process_method=get_tri_skipgram_frequencies_dict)),\n",
    "            (\"tri_skipgram_vectorizer\",DictVectorizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 7_1 is based on postag bi-gram and bi-gram\n",
    "model_7_1 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"postag_bi_gram_features\", Pipeline([\n",
    "            (\"postag_bi_gram_processor\", DocumentProcessor(process_method=get_postag_bi_gram_frequencies_dict)),\n",
    "            (\"postag_bi_gram_vectorizer\", DictVectorizer())\n",
    "        ])),\n",
    "        (\"bi_gram_features\", Pipeline([\n",
    "            (\"bi_gram_processor\", DocumentProcessor(process_method=get_bi_gram_frequencies_dict)),\n",
    "            (\"bi_gram_vectorizer\", DictVectorizer())\n",
    "        ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 7_2 is based on postag tri-gram and tri-gram\n",
    "model_7_2 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"postag_tri_gram_features\", Pipeline([\n",
    "            (\"postag_tri_gram_processor\", DocumentProcessor(process_method=get_postag_tri_gram_frequencies_dict)),\n",
    "            (\"postag_tri_gram_vectorizer\", DictVectorizer())\n",
    "        ])),\n",
    "        (\"tri_gram_features\", Pipeline([\n",
    "            (\"tri_gram_processor\", DocumentProcessor(process_method=get_tri_gram_frequencies_dict)),\n",
    "            (\"tri_gram_vectorizer\", DictVectorizer())\n",
    "        ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 8_1 is based on lemma bi-gram and bi-gram\n",
    "model_8_1 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"lemma_bi_gram_features\",Pipeline([\n",
    "            (\"lemma_bi_gram_processor\",DocumentProcessor(process_method=get_lemma_bi_gram_frequencies_dict)),\n",
    "            (\"lemma_bi_gram_vectorizer\",DictVectorizer())\n",
    "        ])),\n",
    "        (\"bi_gram_features\", Pipeline([\n",
    "            (\"bi_gram_processor\", DocumentProcessor(process_method=get_bi_gram_frequencies_dict)),\n",
    "            (\"bi_gram_vectorizer\", DictVectorizer())\n",
    "        ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])\n",
    "\n",
    "# pipeline model 8_2 is based on lemma tri-gram and tri-gram\n",
    "model_8_2 = Pipeline([\n",
    "    (\"union\",FeatureUnion(transformer_list=[\n",
    "        (\"lemma_tri_gram_features\",Pipeline([\n",
    "            (\"lemma_tri_gram_processor\",DocumentProcessor(process_method=get_lemma_tri_gram_frequencies_dict)),\n",
    "            (\"lemma_tri_gram_vectorizer\",DictVectorizer())\n",
    "        ])),\n",
    "        (\"tri_gram_features\", Pipeline([\n",
    "            (\"tri_gram_processor\", DocumentProcessor(process_method=get_tri_gram_frequencies_dict)),\n",
    "            (\"tri_gram_vectorizer\", DictVectorizer())\n",
    "        ])),\n",
    "    ])),\n",
    "    (\"clf\",LinearSVC(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Give the previous model names and run the function train_model_by_cv(), the results of four metrics would be outputed.\n",
    "\n",
    "def train_model_by_cv(model_name):\n",
    "    time_start = time.time()\n",
    "\n",
    "    X,Y = train_text_data_creation(data_partition=False)\n",
    "\n",
    "    # just return different cv scores based on cross-validation test sets\n",
    "    cv_scores_model = cross_validate(model_name, X, Y, cv=StratifiedKFold(n_splits=10,random_state=0), return_train_score=False,\n",
    "                                   scoring=[\"accuracy\",\"precision_weighted\",\"recall_weighted\",\"f1_weighted\"])\n",
    "    time_end = time.time()\n",
    "    print(\"Time cost: \",time_end - time_start,\"seconds\")\n",
    "    print_cv_scores_summary(cv_scores_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the names of models could be model_3, model_4, model_5, model_6_1, model_6_2, model_7_1, model_7_2, model_8_1, model_8_2\n",
    "train_model_by_cv(model_7_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9. -*- Based on Word2Vec model (CBOW) in gensim package, convert all fake news sentences into vector representations to train models-*-\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# After running the function fake_news_to_vectors(), the trained word2vec CBOW model would be saved. \n",
    "# Just load the saved CBOW model in function vectorized_data_creation(), and use it directly.\n",
    "\n",
    "def fake_news_to_vectors():\n",
    "    # not to part the corpus X and target Y\n",
    "    corpus,Y = train_text_data_creation(data_partition=False)\n",
    "    # get all tokens in each fake news sentences\n",
    "    fake_news_sen_tokens = [doc.tokens_list for doc in corpus]\n",
    "\n",
    "    # train a word2vec CBOW model to convert all tokens in every fake news sentences, into vector representations\n",
    "    # word dimensionality parameter size is set to 300\n",
    "    # Here, in output layer of CBOW neural netword, it would use negatve sampling method to do the gradient ascent (sg=0 and hs=0),\n",
    "    # and the amount of noise word is set to 10.\n",
    "    word2vec_model = word2vec.Word2Vec(sentences=fake_news_sen_tokens, size=300, window=10, min_count=1, sg=0, hs=0,\n",
    "                                       negative=10,seed=0,workers=4)\n",
    "\n",
    "    # The model would not be trained any further. Hence here, use init_sims() function to make model more read-effective and memory-effective\n",
    "    word2vec_model.init_sims(replace=True)\n",
    "    # Svae the trained word2vec CBOW model. Later, for convenience, it can load saved model and get word vectors directly.\n",
    "    word2vec_model.save(\"fake_news_word2vec_model.model\")\n",
    "    print(\"fake_news_word2vec_model is saved successfully\")\n",
    "\n",
    "\n",
    "def vectorized_data_creation(mean_vector = True):\n",
    "    # load saved word2vec CBOW model and get word vectors directly.\n",
    "    word2vec_model = word2vec.Word2Vec.load(\"fake_news_word2vec_model.model\")\n",
    "\n",
    "    # not to part the corpus X and target Y\n",
    "    corpus, Y = train_text_data_creation(data_partition=False)\n",
    "    # get all tokens in each fake news sentences\n",
    "    fake_news_sen_tokens = [doc.tokens_list for doc in corpus]\n",
    "\n",
    "    # initialize a 300-dimensionality numpy array to store all vectors of fake news sentences (each row represents the vector\n",
    "    # of one fake news sentence )\n",
    "    X_vectors = np.zeros((len(fake_news_sen_tokens),300))\n",
    "\n",
    "\n",
    "    for num in range(len(fake_news_sen_tokens)):\n",
    "        each_news_tokens = fake_news_sen_tokens[num]\n",
    "        each_news_vectors = None\n",
    "\n",
    "        if mean_vector:\n",
    "            # # initialize a 300-dimensionality numpy array to store a vector of one fake news sentence\n",
    "            # each_news_vectors = np.zeros((len(each_news_tokens), 300))\n",
    "            #\n",
    "            # for i in range(len(each_news_tokens)):\n",
    "            #     # one raw in zero array each_news_vectors, would be replaced by a 300-dims vector array of one token\n",
    "            #     each_news_vectors[i] = word2vec_model.wv[each_news_tokens[i]]\n",
    "            each_news_vectors = np.vstack((word2vec_model.wv[token] for token in each_news_tokens))\n",
    "\n",
    "            # Now, the numpy array contains massive rows, and rach row represents a 300-dimensionality vector of a token in each fake news sentence\n",
    "            # However here, it is needed to calculate the mean vector of all tokens' vectors in one fake news sentence, and use this mean vector to\n",
    "            # represent the vector of one fake news sentence (sentence vector)\n",
    "            each_news_vectors = np.mean(each_news_vectors, axis=0)\n",
    "\n",
    "        else:\n",
    "            each_news_vectors = np.vstack((word2vec_model.wv[token] for token in each_news_tokens))\n",
    "            each_news_vectors = np.sum(each_news_vectors, axis=0)\n",
    "\n",
    "\n",
    "        # Then, store the 300-dimensionality vector of each fake news sentences into each row of numpy array X_vectors\n",
    "        X_vectors[num] = each_news_vectors\n",
    "\n",
    "    return X_vectors,Y\n",
    "\n",
    "# Based on all vectors of fake news sentences, train a model\n",
    "def train_model_by_vectors():\n",
    "    time_start = time.time()\n",
    "\n",
    "    X_vectors, Y = vectorized_data_creation(mean_vector=False)\n",
    "    linear_svc = LinearSVC(random_state=0)\n",
    "\n",
    "    cv_scores_model = cross_validate(linear_svc, X_vectors, Y, cv=StratifiedKFold(n_splits=10,random_state=0),return_train_score=False,\n",
    "                                   scoring=[\"accuracy\",\"precision_weighted\",\"recall_weighted\",\"f1_weighted\"])\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(\"Time cost: \", time_end - time_start, \"seconds\")\n",
    "    print_cv_scores_summary(cv_scores_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After running the function fake_news_to_vectors(), the trained word2vec CBOW model would be saved.\n",
    "fake_news_to_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the fake news classification by word embedding\n",
    "train_model_by_vectors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
